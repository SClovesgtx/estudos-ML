{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Uma rede de supermercados (normalmente WallMart) usando mineração de dados descobre que há uma estranha correlacão entre compra de cerveja e compra de fraldas (veja mais sobre isso [aqui](https://www.theregister.com/2006/08/15/beer_diapers/)). Em algumas versões a rede coloca um estande de cerveja ao lado das fraldas.\n",
    "\n",
    "As técnicas de mineração de regras de associação e de conjunto de intens (itemset) frequentes é que permitem tirar este tipo de conclusão.\n",
    "\n",
    "Esse notebook surge das minhas anotações pessoais do curso [Pattern Discovery in Data Mining](https://www.coursera.org/learn/data-patterns) junto com materiais complementares (vídeos, artigos, etc) que usei para me ajudar a entender o conteúdo apresentado no curso. Algumas das imagens usadas veem dos slides do curso que mencionei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possibilidades do uso de Regras de Associação\n",
    "\n",
    "Neste tipo de problema há um conjunto de itens (itens num supermercado) e há transações que contem um subconjunto dos itens (uma compra). Mas podemos abstrair isso e generalizar o uso desse recurso. Seguem algumas possibilidades:\n",
    "\n",
    "* os itens podem ser paginas num site, a transação as paginas visitadas em diferentes interações com o site.\n",
    "* o conceito de transação pode não ser localizado no tempo. Pode ser uma pessoa e os itens podem ser aplicativos que essa pessoa instalou no seu celular (não necessariamente ao mesmo tempo).\n",
    "\n",
    "* pode ser proteinas ativas em diferentes tecidos de diferentes individuos (uma transação é a combinação de tecido e individuo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemsets frequentes\n",
    "\n",
    "Itemsets frequentes são conjunto de itens que aparecem juntos em pelo menos *s*% das transações. O número *s*, que precisa ser fornecido para o algoritmo é chamado de suporte.\n",
    "\n",
    "Vamos assumir as seguintes transações:\n",
    "\n",
    "* ABC\n",
    "* AC\n",
    "* CD\n",
    "* AB\n",
    "* BD\n",
    "* D\n",
    "\n",
    "Se o suporte foi definido como 1/3, ou seja queremos conjuntos de itens que aparecem em\n",
    "pelo menos 2 das 6 transações, então AC, AB e D são itemsets frequentes.\n",
    "\n",
    "Um padrão muito grande de intemset contém uma possibilidade de combinações muito grande. Suponha que tenhamos os seguintes itemsets:\n",
    "\n",
    "* $T_1= {a_1, ..., a_{50}}$\n",
    "* $T_2= {a_1, ..., a_{50}, ..., a_{100}}$\n",
    "\n",
    "As possibilidades de combinações de sub-padrões destes itemsets seriam:\n",
    "\n",
    "$(\\sum_{k=1}^{100}{100\\choose k}) - 1= (1 + 1)^{100} -1 = 2^{100} - 1 $\n",
    "\n",
    "Para encontrar todos os intemsets frequentes, seria necessário computar todas essas possibilidades, o que é impossível de fazer em tempo útil. Portando temos algumas outras definições de itemsets que ajudam a diminuir esse número de possibilidades.\n",
    "\n",
    "![](imgs/1.png)\n",
    "\n",
    "\n",
    "Segue as definições de itemsets fechados e maximais:\n",
    "\n",
    "* um itemset *i* é maximal se ele tem suporte maior que *s* e todos os itemsets que incluem *i* tem suporte menor que s, ou seja, não há super padrão Y frequente em que i é sub-conjunto. No exemplo anterior voltar AB, AC e D são maximais. Essa abordagem perde informações pois sabemos apenas que i é frequente mas não o real suporte do mesmo.\n",
    "\n",
    "\n",
    "* Um itemset *i* é fechado (closed) se todos os itemsets que o incluem tem suporte menor que *i*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Propriedade de \"Fechamento para baixo\" dos itemsets frequentes\n",
    "\n",
    "O Algoritmo Apriori assume que:\n",
    "\n",
    "\"*Se um itemset é frequente, todos seus subitemsets também o serão*\"\n",
    "\n",
    "Isso faz sentido, suponha que o itemset $T_1= {a_1, ..., a_{50}}$ seja frequente, então  ${(a_1), (a_1, a_2), ...}$ também serão frequentes. \n",
    "\n",
    "Seguindo esse princípio, deixamos nosso processo de calcular itemsets frequentes mais rápido dado que não precisamos nos preocupar de obter o suporte dos subitemsets de um itemset frequente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori: a abordagem Geração de Candidato & Teste\n",
    "\n",
    "Segue a descrição de cada passo do Algoritmo Apriori:\n",
    "\n",
    "* inicialmente, varrer todo o Banco de Dados (BD) uma vez em busca de itemsets de tamanho 1 que sejam frequentes\n",
    "* Repita:\n",
    "   * Gere itemsets de tamanho (k+1) dos itemsets frequentes de tamanho k\n",
    "   * Teste os itemsets candidatos no BD para achar itemsets frequentes de tamanho (k+1)\n",
    "   * faça k := k + 1\n",
    "* até nenhum conjunto frequente ou candidato puder ser gerado\n",
    "* retorne todos os itemsets frequentes derivados \n",
    "\n",
    "Vamos para uma exemplo concreto.Vamos definir o suporte mínimo como sendo 2 (*minsup = 2*) e que tenhamos um seguinte BD:\n",
    "\n",
    "![](imgs/2.png)\n",
    "\n",
    "O primeiro passo seria varrer todo o BD em busca dos itemsets de tamanho 1 que sejam frequentes. Então teríamos:\n",
    "\n",
    "![](imgs/3.png)\n",
    "\n",
    "Depois entramos num loop gerando candidatos de tamanho k+1 dos itemsetes frequentes de tamanho k:\n",
    "\n",
    "![](imgs/4.png)\n",
    "\n",
    "Fazemos o teste buscando itemsets frequentes aquivalentes aos itemsets candidatos gerados no passo anterior. Os que estão em azul não são frequentes e portanto são eliminados:\n",
    "\n",
    "![](imgs/5.png)\n",
    "\n",
    "![](imgs/6.png)\n",
    "\n",
    "Com esses novos itemsets frequentes de tamanho 2, iremos seguir no loop e vamos gerar itemsets candidatos de tamanho K+1, ou seja, de tamanho 3. Como AB não é frequente, então ABC não será derivado, apenas BCE:\n",
    "\n",
    "![](imgs/7.png)\n",
    "\n",
    "Repetimos o processo de varrer o BD buscando itemsets frequentes aquivalentes aos itemsets candidatos gerados no passo anterior e verificamos que o candidato BCE é frequente:\n",
    "\n",
    "![](imgs/8.png)\n",
    "\n",
    "Terminamos o processo e então retornamos os itemsets frequentes encontrados.\n",
    "\n",
    "Um dos passos fudamentais nesse algoritmo é a geração de candidatos. Como gera-los de maneira eficiente? Uma das maneiras seria seguir os passos listados abaixo: \n",
    "\n",
    "* fazer o auto-agrupamento (abc + bcd = abcd)\n",
    "* podagem (quando um subset do auto-agrupamento não existe no BD)\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "* $F_3 = {abc, abd, acd, ace, bcd}$\n",
    "* auto-agrupamento:\n",
    "   * *abcd* agrupamento que surge das transações *abc* e *abd*\n",
    "   * *acde* agrupamento que surge das transações *acd* e *ace*\n",
    "* podagem:\n",
    "   * *acde* é removido/podado como candidato pois seu subset *ade* não existe em $F_3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori: Melhorias e Alternativas\n",
    "\n",
    "Encontrar o suporte para cada candidato é um processo custoso. Se você tem 10 candidatos, muito provavelmente você terá que varrer todo o BD 10 vezes para encontrar os respectivos suportes.\n",
    "\n",
    "Seguem algumas abordagem sugeridas para diminuir o custo computacional do Algoritmo Apriori:\n",
    "\n",
    "* **Reduzir os passos de varredura sobre o BD**:\n",
    "\n",
    "    * Particionamento (e.g Saravage et al. 1995)\n",
    "    * Contagem dinâmica de Itemsets (Brin et al. 1997)\n",
    "    \n",
    "* **Encolher número de candidatos**:\n",
    "\n",
    "    * Hashing (e.g, DHP: Park et al, 1995)\n",
    "    * Podagem por suporte do limear inferior (e.g Bayardo 1998) \n",
    "    * Amostragem (e.g Toivonen, 1996)\n",
    " \n",
    "* **Exporar estrutura especiais de dados**:\n",
    "\n",
    "    * Projeção de Árvore (Aggarwal et al, 2001)\n",
    "    * Minerador-H (Pei, et al, 2001)\n",
    "    * Decomposição por Hipercubo (e.g, LCM: Uno, et al, 2004)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particionamento: varrer o BD apenas duas vezes\n",
    "\n",
    "Começamos por um teorema:\n",
    "\n",
    "\"*Qualquer itemset que é potencialmente frequente em um BD de transações (BDT) deve ser frequente em pelo menos uma das partições do  BDT.*\"\n",
    "\n",
    "O método de particionamento consiste em:\n",
    "\n",
    "* Particionar o BD\n",
    "* Encontar os itemsets frequentes para cada partição (padrões frequentes locais)\n",
    "* Consolidar os padrões frequentes globais \n",
    "\n",
    "### Hashing Direto e Podagem (DHP)\n",
    "\n",
    "O DHP (Direct Hashing and Pruning) tem por objetivo reduzir o número de candidatos. Primeiro precisamos entender o que é Hashing Direto e há um vídeo bem curto que explica muito bem a ideia por trás dessa abordagem, assista ele [aqui](https://www.youtube.com/watch?v=SwA_pQH0ihQ).\n",
    "\n",
    "A ideia é criar uma tabela com buckets contendo itemsets de tamanho k (k-itemsets):\n",
    "\n",
    "* Cada combinação de k-itemsets candidatos gerado é mapeado para um bucket da Tabela de Hashing Direto e assim a contagem desse bucket é incrementado. \n",
    "\n",
    "* Se um bucket, que um k-itemset está associado, não tiver contagem superior a suporte mínimo, o bucket em questão será \"podado\" e consequentemente os k-itemsets candidatos do bucket em questão não serão considerados frequentes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando Dados de Formato Vertical: ECLAT\n",
    "\n",
    "ECLAT é o acrônimo para \"Equivalente Class Transformation\" e esse algoritmo tenta explorar as vantagem de dados em formato vertical. \n",
    "\n",
    "Um BDT tem o formato horizontal como segue:\n",
    "\n",
    "![](imgs/10.png)\n",
    "\n",
    "Mas ele pode ser transformado para um formato vertical como segue:\n",
    "\n",
    "![](imgs/11.png)\n",
    "\n",
    "Qual a vantagem disso? Podemos pesquisar hiper-itemsets com base em seus sub-itemsets. Por exemplo:\n",
    "\n",
    "* $t(e) = {T_{10}, T_{20}, T_{30}}$; \n",
    "\n",
    "* $t(a) = {T_{10}, T_{20}}$;\n",
    "\n",
    "* $t(ae) = t(e) \\cap t(a) = {T_{10}, T_{20}}$\n",
    "\n",
    "Com isso podemos derivar padrões frequentes. Para acelerar ainda mais o processo de minerar padrões frequentes nesse tipo de estrutura de dados podemos monitorar as diferenças ao invés da interseção, pois o resultado da diferença é menor que a interseção de intemsets frequentes, assim salvamos um pouco de memória:\n",
    "\n",
    "* $ t(ce) = {T_{10}, T_{20}}$\n",
    "\n",
    "* $ difset(ce) = {T_{20}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPGrowth: Minerando Padrões Frequentes pelo Crescimento de Padrões\n",
    "\n",
    "A ideia principal dessa abordagem é que *padrões frequentes crescem*, por isso do nome *Frequent Patterns Growth* (FPGrowth).\n",
    "\n",
    "Seguem os passos desse algoritmo:\n",
    "\n",
    "* Encontre itemsets frequentes de tamanho 1 e particione o BD em cada um de tais 1-itemsets frequentes.\n",
    "* Recursivamente cresça os padrões frequentes aplicando o passo anterior para cada uma dos BDs partiçionados (também conhecidos como BDs particionados)\n",
    "* Para facilitar a eficiência do processamento, uma estrura de dados eficiente chamada FP-tree pode ser usada (veja mais sobre FP-tree [aqui](https://dzone.com/articles/machinex-understanding-fp-tree-construction#:~:text=To%20put%20it%20simply%2C%20an,items%2C%20their%20paths%20may%20overlap.)).\n",
    "\n",
    "Usando as FP-tree o processo de mineração de padrões seria o seguinte:\n",
    "\n",
    "* Recursivamente construa e minere (condicionalmente) as FP-trees.\n",
    "* Até a FP-tree estar vazia, ou até ela conter um path (paths únicos irão gerar todas suas possíveis combinações de sub-paths, cada um deles sendo padrões frequentes).\n",
    "\n",
    "As imagens que usarei a seguir para exemplificar a construção de uma FP-trees eu retirei dos slides do curso [Pattern Discovery in Data Mining](https://www.coursera.org/learn/data-patterns).\n",
    "\n",
    "Vamos supor que temos o seguinte BD de transações:\n",
    "\n",
    "![](imgs/15.png)\n",
    "\n",
    "Seguimos os seguintes passos:\n",
    "\n",
    "* Varrer um BD uma vez para encontrar 1-itemsets frequentes (vamos usar 3 como suporte mínimo). Teremos então: f:4, a:3, c:4, b:3, m:3 e p:3.\n",
    "\n",
    "* Ordene os itemsets frequentes em ordem decrescente: F-list = f-c-a-b-m-p.\n",
    "\n",
    "* Varrer o BD novamente para construir FP-tree.\n",
    "\n",
    "A seguir temos a nossa FP-tree do nosso exemplo de BDT. O 1-itemset *f* aparece 4 vezes, o 2-itemset *f-c*  aparece 3 vezes, o 3-itemset f-c-a aparace 3 vezes, o 4-itemset f-c-a-m aparece 2 vezes, 4-itemset f-c-a-b aparece 1 vez, o 5-itemset f-c-a-m-p aparece 2 vezes, o 5-itemset f-c-a-b-m aparece 1 vezes, etc.\n",
    "\n",
    "![](imgs/16.png)\n",
    "\n",
    "Podemos usar a abordagem recursiva de [Dividir e Conquistar](https://pt.wikipedia.org/wiki/Divis%C3%A3o_e_conquista) para construir essa árvore. Os padrões podem ser particionados de acordo o padrão corrente:\n",
    " \n",
    " * Padrões contendo *p*, ou seja, o BD condicionado a *p*: fcam:2, cb:1\n",
    " * Padrões contendo *m* mas não *p*, ou seja, BD condicionado a m:  fca:2, fcab:1\n",
    " * etc\n",
    " \n",
    "Com isso podemos criar por exemplo uma base dos padrões: \n",
    " \n",
    " ![](imgs/17.png)\n",
    " \n",
    "Para os padrões p-condicionais podemos minerar os 1-itemsets frequentes. Por exemplo: \n",
    "\n",
    "* para a base p-condicional temos o *c*:3 1-itemset frequente pois *fcam*:2 e *cb*:1\n",
    "* para a base m-condicional temos o 3-itemset *fca*:3 pois temos *fca*:2 e *fcab*=1\n",
    "* etc\n",
    "\n",
    "A vantagem dessa estrutura é que num único ramo dessa árvore, todos os seus sub-padrões frequentes podem ser gerados de forma imediata:\n",
    "\n",
    "![](imgs/18.png)\n",
    "\n",
    "O que fazer se a estrutura de dados FP-tree não couber na memória? Pode-se fazer a projeção num BD (DB Projection).\n",
    "\n",
    "Para uma aplicação dessa abordagem usando Python, leia [Understand and Build FP-Growth Algorithm in Python](https://towardsdatascience.com/understand-and-build-fp-growth-algorithm-in-python-d8b989bab342)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de exercício:\n",
    "\n",
    "![](imgs/19.png)\n",
    "\n",
    "![](imgs/20.png)\n",
    "\n",
    "![](imgs/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício de Programação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "In this programming assignment, you are required to implement the Apriori algorithm and apply it to mine frequent itemsets from a real-life data set.\n",
    "\n",
    "### Input\n",
    "\n",
    "The provided input file (\"categories.txt\") consists of the category lists of 77,185 places in the US. Each line corresponds to the category list of one place, where the list consists of a number of category instances (e.g., hotels, restaurants, etc.) that are separated by semicolons.\n",
    "\n",
    "An example line is provided below:\n",
    "\n",
    "\n",
    "```Local Services;IT Services & Computer Repair```\n",
    "\n",
    "In the example above, the corresponding place has two category instances: \"Local Services\" and \"IT Services & Computer Repair\".\n",
    "\n",
    "```categories.txt```\n",
    "\n",
    "### Output\n",
    "\n",
    "You need to implement the Apriori algorithm and use it to mine category sets that are frequent in the input data. When implementing the Apriori algorithm, you may use any programming language you like. We only need your result pattern file, not your source code file.\n",
    "\n",
    "After implementing the Apriori algorithm, please set the relative minimum support to 0.01 and run it on the 77,185 category lists. In other words, you need to extract all the category sets that have an absolute support larger than 771.\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "Please output all the length-1 frequent categories with their absolute supports into a text file named \"patterns.txt\". Every line corresponds to exactly one frequent category and should be in the following format:\n",
    "\n",
    "```support:category```\n",
    "\n",
    "For example, suppose a category (Fast Food) has an absolute support 3000, then the line corresponding to this frequent category set in \"patterns.txt\" should be:\n",
    "\n",
    "```3000:Fast Food```\n",
    "\n",
    "**Part 2**\n",
    "\n",
    "Please write all the frequent category sets along with their absolute supports into a text file named \"patterns.txt\". Every line corresponds to exactly one frequent category set and should be in the following format:\n",
    "\n",
    "\n",
    "```support:category_1;category_2;category_3```\n",
    "\n",
    "\n",
    "For example, suppose a category set (Fast Food; Restaurants) has an absolute support 2851, then the line corresponding to this frequent category set in \"patterns.txt\" should be:\n",
    "\n",
    "```2851:Fast Food;Restaurants```\n",
    "\n",
    "### Important Tips\n",
    "\n",
    "Make sure that you format each line correctly in the output file. For instance, use a semicolon instead of another character to separate the categories for each frequent category set.\n",
    "\n",
    "In the result pattern file, the order of the categories does not matter. For example, the following two cases will be considered equivalent by the grader:\n",
    "\n",
    "**Case 1**:\n",
    "\n",
    "```2851:Fast Food;Restaurants```\n",
    "\n",
    "**Case 2**:\n",
    "\n",
    "```2851:Restaurants;Fast Food```\n",
    "\n",
    "### How to submit\n",
    "\n",
    "When you're ready to submit, you can upload files for each part of the assignment on the \"My submission\" tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Breakfast & Brunch', 'American (Traditional)', 'Restaurants'],\n",
       " ['Sandwiches', 'Restaurants'],\n",
       " ['Local Services', 'IT Services & Computer Repair'],\n",
       " ['Restaurants', 'Italian'],\n",
       " ['Food', 'Coffee & Tea']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/categories.txt\", \"r\") as file:\n",
    "    categories = [line.rstrip().split(\";\") for line in file]\n",
    "categories[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "1-itemsets frequentes (suporte relativo 0.01)\n",
    "\n",
    "A implementação a seguir foi criada por [Andrewngai](https://towardsdatascience.com/@andrewngai9255) e que é apresentada em seu artigo no medium intitulado [Understand and Build FP-Growth Algorithm in Python](https://towardsdatascience.com/understand-and-build-fp-growth-algorithm-in-python-d8b989bab342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node:\n",
    "    def __init__(self, word, word_count=0, parent=None, link=None):\n",
    "        self.word=word\n",
    "        self.word_count=word_count\n",
    "        self.parent=parent\n",
    "        self.link=link\n",
    "        self.children={}\n",
    "\n",
    "#tree traversal\n",
    "    def visittree(self):\n",
    "#        if self is None:\n",
    "#            return None\n",
    "        output=[]\n",
    "        output.append(str(vocabdic[self.word]) + \" \" +str(self.word_count))\n",
    "        if len(list(self.children.keys()))>0:\n",
    "            for i in (list(self.children.keys())):\n",
    "                output.append(self.children[i].visittree())\n",
    "        return output\n",
    "  \n",
    "              \n",
    "'''      Build FPTREE class and method       '''        \n",
    "class fptree:\n",
    "    def __init__(self, data, minsup=400):\n",
    "        #raw data and minminual support\n",
    "        self.data=data\n",
    "        self.minsup=minsup\n",
    "        \n",
    "        #null root\n",
    "        self.root= node(word=\"Null\", word_count=1)\n",
    "        \n",
    "        #each line of transaction with new order from the most frequent items to less\n",
    "        self.wordlinesort=[]\n",
    "        #node table containing link of all nodes of same word\n",
    "        self.nodetable=[]\n",
    "        #dictionary contaiing word more than the minsupport count with des order\n",
    "        self.wordsortdic=[]\n",
    "       \n",
    "        #dictionaly containing word and the support count        \n",
    "        self.worddic={}\n",
    "        #dictionary with word and it's postion of the support count rank\n",
    "        self.wordorderdic={}\n",
    "#        \n",
    "#        self.preprocess(data)\n",
    "#        #first scan to build all the necessay dictionary\n",
    "        self.construct(data)\n",
    "        #second scan and build fp tree line  by line            \n",
    "    def construct(self, data):\n",
    "                #get support count for all word\n",
    "        for tran in data:\n",
    "            for words in tran:\n",
    "                if words in self.worddic.keys():\n",
    "                    self.worddic[words]+=1\n",
    "                else:\n",
    "                    self.worddic[words]=1\n",
    "        wordlist = list(self.worddic.keys())\n",
    "        #prune all the world with < min support count\n",
    "        for word in wordlist:\n",
    "            if(self.worddic[word]<self.minsup):\n",
    "                del self.worddic[word]\n",
    "        #sort the remaing items des, with first word count than work#id        \n",
    "        self.wordsortdic = sorted(self.worddic.items(), key=lambda x: (-x[1],x[0])) \n",
    "        #create a table containing word, wordcount and all link node of that word\n",
    "        t=0\n",
    "        for i in self.wordsortdic:\n",
    "            word = i[0]\n",
    "            wordc = i[1]\n",
    "            self.wordorderdic[word]=t\n",
    "            t+=1\n",
    "            wordinfo = {'wordn':word, 'wordcc':wordc, 'linknode': None}\n",
    "            self.nodetable.append(wordinfo)\n",
    "        #construct fptree line by line\n",
    "    \n",
    "        for line in data:\n",
    "            supword=[]\n",
    "            for word in line:\n",
    "                #only keep words with support count higher than minsupport\n",
    "                if word in self.worddic.keys():\n",
    "                    supword.append(word)\n",
    "           #insert words to the fp tree\n",
    "            if len(supword)>0:\n",
    "                #reorder the words \n",
    "                sortsupword = sorted(supword, key = lambda k: self.wordorderdic[k])\n",
    "                self.wordlinesort.append(sortsupword)\n",
    "                #enter the word one by one from begining\n",
    "                R = self.root\n",
    "#                print(sortsupword)\n",
    "                for i in sortsupword:                  \n",
    "                    if i in R.children.keys():\n",
    "                        R.children[i].word_count +=1\n",
    "                        R=R.children[i]\n",
    "                    else:\n",
    "\n",
    "                        R.children[i] = node(word=i,word_count=1,parent=R,link=None)\n",
    "                        R=R.children[i]\n",
    "                        # link this node to nodetable\n",
    "                        for wordinfo in self.nodetable:\n",
    "                            if wordinfo[\"wordn\"] == R.word:\n",
    "                                # find the last node of the  node linklist\n",
    "                                if wordinfo[\"linknode\"] is None:\n",
    "                                    wordinfo[\"linknode\"] = R\n",
    "                                else:\n",
    "                                    iter_node = wordinfo[\"linknode\"]\n",
    "                                    while(iter_node.link is not None):\n",
    "                                        iter_node = iter_node.link\n",
    "                                    iter_node.link = R\n",
    "\n",
    "# create transactions for conditinal tree   \n",
    "    def condtreetran(self,N):\n",
    "        if N.parent is None:\n",
    "            return None\n",
    "        \n",
    "        condtreeline =[]\n",
    "        #starting from the leaf node reverse add word till hit root\n",
    "        while N is not None:\n",
    "            line=[]\n",
    "            PN = N.parent\n",
    "            while PN.parent is not None:\n",
    "                line.append(PN.word)\n",
    "                PN=PN.parent\n",
    "            #reverse order the transaction\n",
    "            line = line[::-1]\n",
    "            for i in range(N.word_count):\n",
    "                condtreeline.append(line)   \n",
    "            #move on to next linknode\n",
    "            N=N.link\n",
    "        return condtreeline\n",
    "    \n",
    "#Find frequent word list by creating conditional tree\n",
    "    def findfqt(self,parentnode=None):\n",
    "        if len(list(self.root.children.keys()))==0:\n",
    "            return None\n",
    "        result=[]\n",
    "        sup=self.minsup\n",
    "        #starting from the end of nodetable\n",
    "        revtable = self.nodetable[::-1]\n",
    "        for n in revtable:\n",
    "            fqset=[set(),0]\n",
    "            if(parentnode==None):      \n",
    "                fqset[0]={n['wordn'],}\n",
    "            else:\n",
    "                fqset[0] = {n['wordn']}.union(parentnode[0])\n",
    "            fqset[1]=n['wordcc']\n",
    "            result.append(fqset)\n",
    "            condtran = self.condtreetran(n['linknode'])\n",
    "            #recursively build the conditinal fp tree\n",
    "            contree= fptree(condtran,sup)\n",
    "            conwords = contree.findfqt(fqset)\n",
    "            if conwords is not None:\n",
    "                for words in conwords:\n",
    "                    result.append(words)\n",
    "        return result\n",
    "\n",
    "#check if tree hight is larger than 1 \n",
    "    def checkheight(self):\n",
    "        if len(list(self.root.children.keys()))==0:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_tree = fptree(categories, len(categories)*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentitemset = fp_tree.findfqt() # mining frequent patterns\n",
    "frequentitemset = sorted(frequentitemset, key = lambda k: -k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Restaurants'}, 25071],\n",
       " [{'Shopping'}, 11233],\n",
       " [{'Food'}, 9250],\n",
       " [{'Beauty & Spas'}, 6583],\n",
       " [{'Health & Medical'}, 5121],\n",
       " [{'Nightlife'}, 5088],\n",
       " [{'Home Services'}, 4785],\n",
       " [{'Bars'}, 4328],\n",
       " [{'Bars', 'Nightlife'}, 4328],\n",
       " [{'Automotive'}, 4208],\n",
       " [{'Local Services'}, 3468],\n",
       " [{'Active Life'}, 3103],\n",
       " [{'Fashion'}, 3078],\n",
       " [{'Fashion', 'Shopping'}, 3078],\n",
       " [{'Event Planning & Services'}, 2975],\n",
       " [{'Fast Food'}, 2851],\n",
       " [{'Fast Food', 'Restaurants'}, 2851],\n",
       " [{'Pizza'}, 2657],\n",
       " [{'Pizza', 'Restaurants'}, 2657],\n",
       " [{'Nightlife', 'Restaurants'}, 2533],\n",
       " [{'Mexican'}, 2515],\n",
       " [{'Mexican', 'Restaurants'}, 2515],\n",
       " [{'Hotels & Travel'}, 2495],\n",
       " [{'Bars', 'Restaurants'}, 2423],\n",
       " [{'Bars', 'Nightlife', 'Restaurants'}, 2423],\n",
       " [{'American (Traditional)'}, 2416],\n",
       " [{'American (Traditional)', 'Restaurants'}, 2416],\n",
       " [{'Sandwiches'}, 2364],\n",
       " [{'Restaurants', 'Sandwiches'}, 2364],\n",
       " [{'Arts & Entertainment'}, 2271],\n",
       " [{'Coffee & Tea'}, 2199],\n",
       " [{'Coffee & Tea', 'Food'}, 2199],\n",
       " [{'Food', 'Restaurants'}, 2101],\n",
       " [{'Hair Salons'}, 2091],\n",
       " [{'Beauty & Spas', 'Hair Salons'}, 2091],\n",
       " [{'Italian'}, 1848],\n",
       " [{'Italian', 'Restaurants'}, 1848],\n",
       " [{'Burgers'}, 1774],\n",
       " [{'Burgers', 'Restaurants'}, 1774],\n",
       " [{'Auto Repair'}, 1716],\n",
       " [{'Auto Repair', 'Automotive'}, 1716],\n",
       " [{'Doctors'}, 1694],\n",
       " [{'Doctors', 'Health & Medical'}, 1694],\n",
       " [{'Nail Salons'}, 1667],\n",
       " [{'Beauty & Spas', 'Nail Salons'}, 1667],\n",
       " [{'Chinese'}, 1629],\n",
       " [{'Chinese', 'Restaurants'}, 1629],\n",
       " [{'American (New)'}, 1593],\n",
       " [{'American (New)', 'Restaurants'}, 1593],\n",
       " [{'Home & Garden'}, 1586],\n",
       " [{'Home & Garden', 'Shopping'}, 1586],\n",
       " [{'Pets'}, 1497],\n",
       " [{'Event Planning & Services', 'Hotels & Travel'}, 1471],\n",
       " [{'Fitness & Instruction'}, 1442],\n",
       " [{'Active Life', 'Fitness & Instruction'}, 1442],\n",
       " [{'Hotels'}, 1431],\n",
       " [{'Hotels', 'Hotels & Travel'}, 1431],\n",
       " [{'Event Planning & Services', 'Hotels', 'Hotels & Travel'}, 1431],\n",
       " [{'Event Planning & Services', 'Hotels'}, 1431],\n",
       " [{'Real Estate'}, 1424],\n",
       " [{'Home Services', 'Real Estate'}, 1424],\n",
       " [{'Grocery'}, 1424],\n",
       " [{'Food', 'Grocery'}, 1424],\n",
       " [{'Breakfast & Brunch'}, 1369],\n",
       " [{'Breakfast & Brunch', 'Restaurants'}, 1369],\n",
       " [{'Dentists'}, 1195],\n",
       " [{'Dentists', 'Health & Medical'}, 1195],\n",
       " [{'Specialty Food'}, 1150],\n",
       " [{'Food', 'Specialty Food'}, 1150],\n",
       " [{\"Women's Clothing\"}, 1138],\n",
       " [{'Shopping', \"Women's Clothing\"}, 1138],\n",
       " [{'Fashion', 'Shopping', \"Women's Clothing\"}, 1138],\n",
       " [{'Fashion', \"Women's Clothing\"}, 1138],\n",
       " [{'Bakeries'}, 1115],\n",
       " [{'Bakeries', 'Food'}, 1115],\n",
       " [{'Professional Services'}, 1025],\n",
       " [{'Ice Cream & Frozen Yogurt'}, 1018],\n",
       " [{'Food', 'Ice Cream & Frozen Yogurt'}, 1018],\n",
       " [{'Cafes'}, 1002],\n",
       " [{'Cafes', 'Restaurants'}, 1002],\n",
       " [{'Financial Services'}, 875],\n",
       " [{'Pubs'}, 874],\n",
       " [{'Nightlife', 'Pubs'}, 874],\n",
       " [{'Bars', 'Nightlife', 'Pubs'}, 874],\n",
       " [{'Bars', 'Pubs'}, 874],\n",
       " [{'Pet Services'}, 870],\n",
       " [{'Pet Services', 'Pets'}, 870],\n",
       " [{'Japanese'}, 848],\n",
       " [{'Japanese', 'Restaurants'}, 848],\n",
       " [{'General Dentistry'}, 823],\n",
       " [{'General Dentistry', 'Health & Medical'}, 823],\n",
       " [{'Dentists', 'General Dentistry', 'Health & Medical'}, 823],\n",
       " [{'Dentists', 'General Dentistry'}, 823],\n",
       " [{'Sports Bars'}, 818],\n",
       " [{'Nightlife', 'Sports Bars'}, 818],\n",
       " [{'Bars', 'Nightlife', 'Sports Bars'}, 818],\n",
       " [{'Bars', 'Sports Bars'}, 818],\n",
       " [{'Sushi Bars'}, 798],\n",
       " [{'Restaurants', 'Sushi Bars'}, 798],\n",
       " [{'Burgers', 'Fast Food'}, 774],\n",
       " [{'Burgers', 'Fast Food', 'Restaurants'}, 774]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequentitemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o resultado no Formato Especificado no EP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25071:Restaurants\n",
      "\n",
      "11233:Shopping\n",
      "\n",
      "9250:Food\n",
      "\n",
      "6583:Beauty & Spas\n",
      "\n",
      "5121:Health & Medical\n",
      "\n",
      "5088:Nightlife\n",
      "\n",
      "4785:Home Services\n",
      "\n",
      "4328:Bars\n",
      "\n",
      "4208:Automotive\n",
      "\n",
      "3468:Local Services\n",
      "\n",
      "3103:Active Life\n",
      "\n",
      "3078:Fashion\n",
      "\n",
      "2975:Event Planning & Services\n",
      "\n",
      "2851:Fast Food\n",
      "\n",
      "2657:Pizza\n",
      "\n",
      "2515:Mexican\n",
      "\n",
      "2495:Hotels & Travel\n",
      "\n",
      "2416:American (Traditional)\n",
      "\n",
      "2364:Sandwiches\n",
      "\n",
      "2271:Arts & Entertainment\n",
      "\n",
      "2199:Coffee & Tea\n",
      "\n",
      "2091:Hair Salons\n",
      "\n",
      "1848:Italian\n",
      "\n",
      "1774:Burgers\n",
      "\n",
      "1716:Auto Repair\n",
      "\n",
      "1694:Doctors\n",
      "\n",
      "1667:Nail Salons\n",
      "\n",
      "1629:Chinese\n",
      "\n",
      "1593:American (New)\n",
      "\n",
      "1586:Home & Garden\n",
      "\n",
      "1497:Pets\n",
      "\n",
      "1442:Fitness & Instruction\n",
      "\n",
      "1431:Hotels\n",
      "\n",
      "1424:Real Estate\n",
      "\n",
      "1424:Grocery\n",
      "\n",
      "1369:Breakfast & Brunch\n",
      "\n",
      "1195:Dentists\n",
      "\n",
      "1150:Specialty Food\n",
      "\n",
      "1138:Women's Clothing\n",
      "\n",
      "1115:Bakeries\n",
      "\n",
      "1025:Professional Services\n",
      "\n",
      "1018:Ice Cream & Frozen Yogurt\n",
      "\n",
      "1002:Cafes\n",
      "\n",
      "875:Financial Services\n",
      "\n",
      "874:Pubs\n",
      "\n",
      "870:Pet Services\n",
      "\n",
      "848:Japanese\n",
      "\n",
      "823:General Dentistry\n",
      "\n",
      "818:Sports Bars\n",
      "\n",
      "798:Sushi Bars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parte 1 do ep \n",
    "file = open(\"patterns.txt\", \"w\")\n",
    "\n",
    "for k_itemset in frequentitemset:\n",
    "    if not len(k_itemset[0]) > 1:\n",
    "        one_itemset =  str(k_itemset[1]) + \":\" + \";\".join(k_itemset[0]) + \"\\n\"\n",
    "        print(one_itemset)\n",
    "        file.write(one_itemset)\n",
    "    \n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25071:Restaurants\n",
      "\n",
      "11233:Shopping\n",
      "\n",
      "9250:Food\n",
      "\n",
      "6583:Beauty & Spas\n",
      "\n",
      "5121:Health & Medical\n",
      "\n",
      "5088:Nightlife\n",
      "\n",
      "4785:Home Services\n",
      "\n",
      "4328:Bars\n",
      "\n",
      "4328:Bars;Nightlife\n",
      "\n",
      "4208:Automotive\n",
      "\n",
      "3468:Local Services\n",
      "\n",
      "3103:Active Life\n",
      "\n",
      "3078:Fashion\n",
      "\n",
      "3078:Fashion;Shopping\n",
      "\n",
      "2975:Event Planning & Services\n",
      "\n",
      "2851:Fast Food\n",
      "\n",
      "2851:Restaurants;Fast Food\n",
      "\n",
      "2657:Pizza\n",
      "\n",
      "2657:Restaurants;Pizza\n",
      "\n",
      "2533:Restaurants;Nightlife\n",
      "\n",
      "2515:Mexican\n",
      "\n",
      "2515:Restaurants;Mexican\n",
      "\n",
      "2495:Hotels & Travel\n",
      "\n",
      "2423:Restaurants;Bars\n",
      "\n",
      "2423:Restaurants;Bars;Nightlife\n",
      "\n",
      "2416:American (Traditional)\n",
      "\n",
      "2416:Restaurants;American (Traditional)\n",
      "\n",
      "2364:Sandwiches\n",
      "\n",
      "2364:Restaurants;Sandwiches\n",
      "\n",
      "2271:Arts & Entertainment\n",
      "\n",
      "2199:Coffee & Tea\n",
      "\n",
      "2199:Food;Coffee & Tea\n",
      "\n",
      "2101:Restaurants;Food\n",
      "\n",
      "2091:Hair Salons\n",
      "\n",
      "2091:Hair Salons;Beauty & Spas\n",
      "\n",
      "1848:Italian\n",
      "\n",
      "1848:Restaurants;Italian\n",
      "\n",
      "1774:Burgers\n",
      "\n",
      "1774:Restaurants;Burgers\n",
      "\n",
      "1716:Auto Repair\n",
      "\n",
      "1716:Auto Repair;Automotive\n",
      "\n",
      "1694:Doctors\n",
      "\n",
      "1694:Doctors;Health & Medical\n",
      "\n",
      "1667:Nail Salons\n",
      "\n",
      "1667:Nail Salons;Beauty & Spas\n",
      "\n",
      "1629:Chinese\n",
      "\n",
      "1629:Restaurants;Chinese\n",
      "\n",
      "1593:American (New)\n",
      "\n",
      "1593:Restaurants;American (New)\n",
      "\n",
      "1586:Home & Garden\n",
      "\n",
      "1586:Shopping;Home & Garden\n",
      "\n",
      "1497:Pets\n",
      "\n",
      "1471:Event Planning & Services;Hotels & Travel\n",
      "\n",
      "1442:Fitness & Instruction\n",
      "\n",
      "1442:Fitness & Instruction;Active Life\n",
      "\n",
      "1431:Hotels\n",
      "\n",
      "1431:Hotels;Hotels & Travel\n",
      "\n",
      "1431:Hotels;Event Planning & Services;Hotels & Travel\n",
      "\n",
      "1431:Hotels;Event Planning & Services\n",
      "\n",
      "1424:Real Estate\n",
      "\n",
      "1424:Real Estate;Home Services\n",
      "\n",
      "1424:Grocery\n",
      "\n",
      "1424:Food;Grocery\n",
      "\n",
      "1369:Breakfast & Brunch\n",
      "\n",
      "1369:Restaurants;Breakfast & Brunch\n",
      "\n",
      "1195:Dentists\n",
      "\n",
      "1195:Dentists;Health & Medical\n",
      "\n",
      "1150:Specialty Food\n",
      "\n",
      "1150:Food;Specialty Food\n",
      "\n",
      "1138:Women's Clothing\n",
      "\n",
      "1138:Shopping;Women's Clothing\n",
      "\n",
      "1138:Women's Clothing;Fashion;Shopping\n",
      "\n",
      "1138:Fashion;Women's Clothing\n",
      "\n",
      "1115:Bakeries\n",
      "\n",
      "1115:Food;Bakeries\n",
      "\n",
      "1025:Professional Services\n",
      "\n",
      "1018:Ice Cream & Frozen Yogurt\n",
      "\n",
      "1018:Food;Ice Cream & Frozen Yogurt\n",
      "\n",
      "1002:Cafes\n",
      "\n",
      "1002:Restaurants;Cafes\n",
      "\n",
      "875:Financial Services\n",
      "\n",
      "874:Pubs\n",
      "\n",
      "874:Nightlife;Pubs\n",
      "\n",
      "874:Bars;Nightlife;Pubs\n",
      "\n",
      "874:Bars;Pubs\n",
      "\n",
      "870:Pet Services\n",
      "\n",
      "870:Pets;Pet Services\n",
      "\n",
      "848:Japanese\n",
      "\n",
      "848:Restaurants;Japanese\n",
      "\n",
      "823:General Dentistry\n",
      "\n",
      "823:General Dentistry;Health & Medical\n",
      "\n",
      "823:Dentists;General Dentistry;Health & Medical\n",
      "\n",
      "823:Dentists;General Dentistry\n",
      "\n",
      "818:Sports Bars\n",
      "\n",
      "818:Nightlife;Sports Bars\n",
      "\n",
      "818:Bars;Nightlife;Sports Bars\n",
      "\n",
      "818:Bars;Sports Bars\n",
      "\n",
      "798:Sushi Bars\n",
      "\n",
      "798:Restaurants;Sushi Bars\n",
      "\n",
      "774:Burgers;Fast Food\n",
      "\n",
      "774:Restaurants;Burgers;Fast Food\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parte 1 do ep \n",
    "file = open(\"patterns.txt\", \"w\")\n",
    "for k_itemset in frequentitemset:\n",
    "    freq_itemset =  str(k_itemset[1]) + \":\" + \";\".join(k_itemset[0]) + \"\\n\"\n",
    "    print(freq_itemset)\n",
    "    file.write(freq_itemset)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
