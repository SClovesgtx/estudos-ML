{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Uma rede de supermercados (normalmente WallMart) usando mineração de dados descobre que há uma estranha correlacão entre compra de cerveja e compra de fraldas (veja mais sobre isso [aqui](https://www.theregister.com/2006/08/15/beer_diapers/)). Em algumas versões a rede coloca um estande de cerveja ao lado das fraldas.\n",
    "\n",
    "As técnicas de mineração de regras de associação e de conjunto de intens (itemset) frequentes é que permitem tirar este tipo de conclusão.\n",
    "\n",
    "Esse notebook surge das minhas anotações pessoais do curso [Pattern Discovery in Data Mining](https://www.coursera.org/learn/data-patterns) junto com materiais complementares (vídeos, artigos, etc) que usei para me ajudar a entender o conteúdo apresentado no curso. Algumas das imagens usadas veem dos slides do curso que mencionei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possibilidades do uso de Regras de Associação\n",
    "\n",
    "Neste tipo de problema há um conjunto de itens (itens num supermercado) e há transações que contem um subconjunto dos itens (uma compra). Mas podemos abstrair isso e generalizar o uso desse recurso. Seguem algumas possibilidades:\n",
    "\n",
    "* os itens podem ser paginas num site, a transação as paginas visitadas em diferentes interações com o site.\n",
    "* o conceito de transação pode não ser localizado no tempo. Pode ser uma pessoa e os itens podem ser aplicativos que essa pessoa instalou no seu celular (não necessariamente ao mesmo tempo).\n",
    "\n",
    "* pode ser proteinas ativas em diferentes tecidos de diferentes individuos (uma transação é a combinação de tecido e individuo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemsets frequentes\n",
    "\n",
    "Itemsets frequentes são conjunto de itens que aparecem juntos em pelo menos *s*% das transações. O número *s*, que precisa ser fornecido para o algoritmo é chamado de suporte.\n",
    "\n",
    "Vamos assumir as seguintes transações:\n",
    "\n",
    "* ABC\n",
    "* AC\n",
    "* CD\n",
    "* AB\n",
    "* BD\n",
    "* D\n",
    "\n",
    "Se o suporte foi definido como 1/3, ou seja queremos conjuntos de itens que aparecem em\n",
    "pelo menos 2 das 6 transações, então AC, AB e D são itemsets frequentes.\n",
    "\n",
    "Um padrão muito grande de intemset contém uma possibilidade de combinações muito grande. Suponha que tenhamos os seguintes itemsets:\n",
    "\n",
    "* $T_1= {a_1, ..., a_{50}}$\n",
    "* $T_2= {a_1, ..., a_{50}, ..., a_{100}}$\n",
    "\n",
    "As possibilidades de combinações de sub-padrões destes itemsets seriam:\n",
    "\n",
    "$(\\sum_{k=1}^{100}{100\\choose k}) - 1= (1 + 1)^{100} -1 = 2^{100} - 1 $\n",
    "\n",
    "Para encontrar todos os intemsets frequentes, seria necessário computar todas essas possibilidades, o que é impossível de fazer em tempo útil. Portando temos algumas outras definições de itemsets que ajudam a diminuir esse número de possibilidades.\n",
    "\n",
    "![](imgs/1.png)\n",
    "\n",
    "\n",
    "Segue as definições de itemsets fechados e maximais:\n",
    "\n",
    "* um itemset *i* é maximal se ele tem suporte maior que *s* e todos os itemsets que incluem *i* tem suporte menor que s, ou seja, não há super padrão Y frequente em que i é sub-conjunto. No exemplo anterior voltar AB, AC e D são maximais. Essa abordagem perde informações pois sabemos apenas que i é frequente mas não o real suporte do mesmo.\n",
    "\n",
    "\n",
    "* Um itemset *i* é fechado (closed) se todos os itemsets que o incluem tem suporte menor que *i*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Propriedade de \"Fechamento para baixo\" dos itemsets frequentes\n",
    "\n",
    "O Algoritmo Apriori assume que:\n",
    "\n",
    "\"*Se um itemset é frequente, todos seus subitemsets também o serão*\"\n",
    "\n",
    "Isso faz sentido, suponha que o itemset $T_1= {a_1, ..., a_{50}}$ seja frequente, então  ${(a_1), (a_1, a_2), ...}$ também serão frequentes. \n",
    "\n",
    "Seguindo esse princípio, deixamos nosso processo de calcular itemsets frequentes mais rápido dado que não precisamos nos preocupar de obter o suporte dos subitemsets de um itemset frequente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori: a abordagem Geração de Candidato & Teste\n",
    "\n",
    "Segue a descrição de cada passo do Algoritmo Apriori:\n",
    "\n",
    "* inicialmente, varrer todo o Banco de Dados (BD) uma vez em busca de itemsets de tamanho 1 que sejam frequentes\n",
    "* Repita:\n",
    "   * Gere itemsets de tamanho (k+1) dos itemsets frequentes de tamanho k\n",
    "   * Teste os itemsets candidatos no BD para achar itemsets frequentes de tamanho (k+1)\n",
    "   * faça k := k + 1\n",
    "* até nenhum conjunto frequente ou candidato puder ser gerado\n",
    "* retorne todos os itemsets frequentes derivados \n",
    "\n",
    "Vamos para uma exemplo concreto.Vamos definir o suporte mínimo como sendo 2 (*minsup = 2*) e que tenhamos um seguinte BD:\n",
    "\n",
    "![](imgs/2.png)\n",
    "\n",
    "O primeiro passo seria varrer todo o BD em busca dos itemsets de tamanho 1 que sejam frequentes. Então teríamos:\n",
    "\n",
    "![](imgs/3.png)\n",
    "\n",
    "Depois entramos num loop gerando candidatos de tamanho k+1 dos itemsetes frequentes de tamanho k:\n",
    "\n",
    "![](imgs/4.png)\n",
    "\n",
    "Fazemos o teste buscando itemsets frequentes aquivalentes aos itemsets candidatos gerados no passo anterior. Os que estão em azul não são frequentes e portanto são eliminados:\n",
    "\n",
    "![](imgs/5.png)\n",
    "\n",
    "![](imgs/6.png)\n",
    "\n",
    "Com esses novos itemsets frequentes de tamanho 2, iremos seguir no loop e vamos gerar itemsets candidatos de tamanho K+1, ou seja, de tamanho 3. Como AB não é frequente, então ABC não será derivado, apenas BCE:\n",
    "\n",
    "![](imgs/7.png)\n",
    "\n",
    "Repetimos o processo de varrer o BD buscando itemsets frequentes aquivalentes aos itemsets candidatos gerados no passo anterior e verificamos que o candidato BCE é frequente:\n",
    "\n",
    "![](imgs/8.png)\n",
    "\n",
    "Terminamos o processo e então retornamos os itemsets frequentes encontrados.\n",
    "\n",
    "Um dos passos fudamentais nesse algoritmo é a geração de candidatos. Como gera-los de maneira eficiente? Uma das maneiras seria seguir os passos listados abaixo: \n",
    "\n",
    "* fazer o auto-agrupamento (abc + bcd = abcd)\n",
    "* podagem (quando um subset do auto-agrupamento não existe no BD)\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "* $F_3 = {abc, abd, acd, ace, bcd}$\n",
    "* auto-agrupamento:\n",
    "   * *abcd* agrupamento que surge das transações *abc* e *abd*\n",
    "   * *acde* agrupamento que surge das transações *acd* e *ace*\n",
    "* podagem:\n",
    "   * *acde* é removido/podado como candidato pois seu subset *ade* não existe em $F_3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori: Melhorias e Alternativas\n",
    "\n",
    "Encontrar o suporte para cada candidato é um processo custoso. Se você tem 10 candidatos, muito provavelmente você terá que varrer todo o BD 10 vezes para encontrar os respectivos suportes.\n",
    "\n",
    "Seguem algumas abordagem sugeridas para diminuir o custo computacional do Algoritmo Apriori:\n",
    "\n",
    "* **Reduzir os passos de varredura sobre o BD**:\n",
    "\n",
    "    * Particionamento (e.g Saravage et al. 1995)\n",
    "    * Contagem dinâmica de Itemsets (Brin et al. 1997)\n",
    "    \n",
    "* **Encolher número de candidatos**:\n",
    "\n",
    "    * Hashing (e.g, DHP: Park et al, 1995)\n",
    "    * Podagem por suporte do limear inferior (e.g Bayardo 1998) \n",
    "    * Amostragem (e.g Toivonen, 1996)\n",
    " \n",
    "* **Exporar estrutura especiais de dados**:\n",
    "\n",
    "    * Projeção de Árvore (Aggarwal et al, 2001)\n",
    "    * Minerador-H (Pei, et al, 2001)\n",
    "    * Decomposição por Hipercubo (e.g, LCM: Uno, et al, 2004)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particionamento: varrer o BD apenas duas vezes\n",
    "\n",
    "Começamos por um teorema:\n",
    "\n",
    "\"*Qualquer itemset que é potencialmente frequente em um BD de transações (BDT) deve ser frequente em pelo menos uma das partições do  BDT.*\"\n",
    "\n",
    "O método de particionamento consiste em:\n",
    "\n",
    "* Particionar o BD\n",
    "* Encontar os itemsets frequentes para cada partição (padrões frequentes locais)\n",
    "* Consolidar os padrões frequentes globais \n",
    "\n",
    "### Hashing Direto e Podagem (DHP)\n",
    "\n",
    "O DHP (Direct Hashing and Pruning) tem por objetivo reduzir o número de candidatos. Primeiro precisamos entender o que é Hashing Direto e há um vídeo bem curto que explica muito bem a ideia por trás dessa abordagem, assista ele [aqui](https://www.youtube.com/watch?v=SwA_pQH0ihQ).\n",
    "\n",
    "A ideia é criar uma tabela com buckets contendo itemsets de tamanho k (k-itemsets):\n",
    "\n",
    "* Cada combinação de k-itemsets candidatos gerado é mapeado para um bucket da Tabela de Hashing Direto e assim a contagem desse bucket é incrementado. \n",
    "\n",
    "* Se um bucket, que um k-itemset está associado, não tiver contagem superior a suporte mínimo, o bucket em questão será \"podado\" e consequentemente os k-itemsets candidatos do bucket em questão não serão considerados frequentes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando Dados de Formato Vertical: ECLAT\n",
    "\n",
    "ECLAT é o acrônimo para \"Equivalente Class Transformation\" e esse algoritmo tenta explorar as vantagem de dados em formato vertical. \n",
    "\n",
    "Um BDT tem o formato horizontal como segue:\n",
    "\n",
    "![](imgs/10.png)\n",
    "\n",
    "Mas ele pode ser transformado para um formato vertical como segue:\n",
    "\n",
    "![](imgs/11.png)\n",
    "\n",
    "Qual a vantagem disso? Podemos pesquisar hiper-itemsets com base em seus sub-itemsets. Por exemplo:\n",
    "\n",
    "* $t(e) = {T_{10}, T_{20}, T_{30}}$; \n",
    "\n",
    "* $t(a) = {T_{10}, T_{20}}$;\n",
    "\n",
    "* $t(ae) = t(e) \\cap t(a) = {T_{10}, T_{20}}$\n",
    "\n",
    "Com isso podemos derivar padrões frequentes. Para acelerar ainda mais o processo de minerar padrões frequentes nesse tipo de estrutura de dados podemos monitorar as diferenças ao invés da interseção, pois o resultado da diferença é menor que a interseção de intemsets frequentes, assim salvamos um pouco de memória:\n",
    "\n",
    "* $ t(ce) = {T_{10}, T_{20}}$\n",
    "\n",
    "* $ difset(ce) = {T_{20}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPGrowth: Minerando Padrões Frequentes pelo Crescimento de Padrões\n",
    "\n",
    "A ideia principal dessa abordagem é que *padrões frequentes crescem*, por isso do nome *Frequent Patterns Growth* (FPGrowth).\n",
    "\n",
    "Seguem os passos desse algoritmo:\n",
    "\n",
    "* Encontre itemsets frequentes de tamanho 1 e particione o BD em cada um de tais 1-itemsets frequentes.\n",
    "* Recursivamente cresça os padrões frequentes aplicando o passo anterior para cada uma dos BDs partiçionados (também conhecidos como BDs particionados)\n",
    "* Para facilitar a eficiência do processamento, uma estrura de dados eficiente chamada FP-tree pode ser usada (veja mais sobre FP-tree [aqui](https://dzone.com/articles/machinex-understanding-fp-tree-construction#:~:text=To%20put%20it%20simply%2C%20an,items%2C%20their%20paths%20may%20overlap.)).\n",
    "\n",
    "Usando as FP-tree o processo de mineração de padrões seria o seguinte:\n",
    "\n",
    "* Recursivamente construa e minere (condicionalmente) as FP-trees.\n",
    "* Até a FP-tree estar vazia, ou até ela conter um path (paths únicos irão gerar todas suas possíveis combinações de sub-paths, cada um deles sendo padrões frequentes).\n",
    "\n",
    "As imagens que usarei a seguir para exemplificar a construção de uma FP-trees eu retirei dos slides do curso [Pattern Discovery in Data Mining](https://www.coursera.org/learn/data-patterns).\n",
    "\n",
    "Vamos supor que temos o seguinte BD de transações:\n",
    "\n",
    "![](imgs/15.png)\n",
    "\n",
    "Seguimos os seguintes passos:\n",
    "\n",
    "* Varrer um BD uma vez para encontrar 1-itemsets frequentes (vamos usar 3 como suporte mínimo). Teremos então: f:4, a:3, c:4, b:3, m:3\n",
    "\n",
    "\n",
    "Para uma aplicação dessa abordagem usando Python, leia [Understand and Build FP-Growth Algorithm in Python](https://towardsdatascience.com/understand-and-build-fp-growth-algorithm-in-python-d8b989bab342)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
